<!DOCTYPE HTML>
<html lang="en">
  
<head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
  /* @import url(https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons); */
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/preview.jpg">
  <title>Sayan Deb Sarkar - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sayan Deb Sarkar</name>
              </p>
              <p>I am a Computer Science MSc student at <a href="https://ethz.ch/de.html">ETH Zurich</a>, 
                majoring in Visual Interactive Computing. 
                Recenlty, I completed a research intership in the Perception team at <a href="https://www.qualcomm.com/research/extended-reality">Qualcomm XR Labs</a> hosted by 
                <a href="https://scholar.google.it/citations?user=I8yGMAEAAAAJ&hl=it">Marco Manfredi</a>, optimising SLAM algorithms for real-time performance and
                integrating deep learning algorithms for improved tracking in adversarial scenarios. At ETHZ, I am a part of the <a href="https://cvg.ethz.ch/">Computer Vision and Geometry Group</a> 
                headed by <a href="https://people.inf.ethz.ch/marc.pollefeys/">Prof. Marc Pollefeys</a>, where I work closely with
                <a href="https://scholar.google.com/citations?user=U9-D8DYAAAAJ&hl=en">Dr. Daniel Barath</a>, <a href="https://miksik.co.uk/">Dr. Ondrej Miksik</a> 
                (Microsoft Mixed Reality & AI Labs, Zurich) 
                and <a href="https://ir0.github.io/">Prof. Iro Armeni  </a> (Stanford University)
                on aligning real-world 3D environments from multi-modal data and improving design processes using data-driven methods.
              </p>
              
              <p>
                Before starting MSc, I gained experience working as a Computer Vision Research Engineer at <strong>Mercedes-Benz R&D</strong>,
                on Intelligent Interior Camera Systems, for around a year, and as a Research Assistant for the
                other one and a half years with <a href="https://vincentlepetit.github.io/">Prof. 
                Vincent Lepetit</a> at the <strong>Institute of Computer Graphics and Vision, TU Graz</strong>.
                My work has been published multiple times in top-tier Vision conferences such as CVPR, ICCV and ECCV.
              </p>
              <p>
                I am always looking for interesting research collaborations and ideas, 
                please get in touch via <a href="mailto:sdebsarkar@ethz.ch">email</a> for any such opportunities. 
                Also, if you're in or around Zurich, feel free to reach out, I am always up
                for a good cup of coffee!
              </p>
              <p style="text-align:center">
                <a href="data/Resume_SayanDebSarkar.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=T9zPzwoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/debsarkar_sayan">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/sayands/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sayands/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/sayandebsarkar.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="media/ethz_logo.png" width="150"></a>
            </td>
            <td width="25%" valign="middle">
              <a href="https://www.qualcomm.com/research/extended-reality"><img src="media/qualcomm_logo.png" width="140"></a>
            </td>
            <td width="25%" valign="middle">
              <a href="https://www.mbrdi.co.in"><img src="media/benz_logo.png" width="140"></a>
            </td>
            <td width="25%" valign="middle">
              <a href="https://www.tugraz.at/institute/icg/home"><img src="media/tug_logo.png" width="140"></a>
            </td>
            <td width="25%" valign="middle">
              <a href="https://manipal.edu/mu.html"><img src="media/manipal_logo.svg" width="140"></a>
            </td>
          </tr>
        </table>

        <!-- NEWS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Recent News</heading>
                <ul>
                  <li><strong>01/2024</strong> Back to Zurich, final push to graduate with a Masters degree! </li>
                  <li><strong>07/2023</strong> Started a research internship at Qualcomm XR Labs, Amsterdam </li>
                  <li><strong>07/2023</strong> <a href="https://sayandebsarkar.com/sgaligner/">SGAligner</a> accepted to ICCV 2023, my first <strong>first-author</strong> submission! </li>
                  <li><strong>04/2023</strong> We're organising the workshop <strong>
                    <a href="https://cv4aec.github.io/"> CV4AEC @ CVPR 2023 </a> </strong>, 
                    participate in the <a href="https://codalab.lisn.upsaclay.fr/competitions/12386">2D</a> and 
                    <a href="https://codalab.lisn.upsaclay.fr/competitions/12405">3D</a> challenges! </li>
                  <li>10/2022</strong> Started working at CVG on scene understanding </li>
                  <li><strong>09/2022</strong> Moved to Zurich! started MSc at ETH </li>
                  <li><strong>07/2022</strong> <a href="https://arxiv.org/abs/2104.14639">Keypoint Transformer</a> accepted at CVPR 2022 as an 
                  <strong>Oral</strong> presentation </li>
                  <li><strong>05/2021</strong> Started at <strong>Mercedes-Benz R & D</strong> as a Computer Vision Research Engineer! </li>
                  <a href="javascript:void(0);" onclick="toggleBlock('old_news')">---- show more ----</a>
                  <div id="old_news" style="display: none;">
                    <li><strong>03/2021</strong> <a href="https://arxiv.org/abs/2103.07969">MCSS</a> accepted at CVPR 2021 </li>
                      <li><strong>07/2020</strong> Graduated with a B.Tech from Manipal!</li>
                      <li><strong>06/2020</strong> <a href="https://arxiv.org/abs/2001.02149">General 3D Room Layout from a Single View by Render-and-Compare
                      </a> accepted at ECCV 2020 </li>
                      <li><strong>01/2020</strong> Started as a Research Assistant at TUG! Moved to Graz, Austria</li>
                  </div>
                </div></div>
                </ul>
              </td>
            </tr>
        </tbody></table>

        <!-- PUBLICATIONS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Research</heading>
                <p>
                  My research interests lie at the intersection 
                of Computer Vision and Machine Learning, specifically in the areas of 3D scene understanding and pose estimation. 
                </p>
                
              </p>
              </td>
            </tr>
          </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()" bgcolor="#ffffd0">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="three">
                  <img src='images/sgaligner_teaser.png' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>SGAligner : 3D Scene Alignment with Scene Graphs</papertitle>
                <br>
                <strong>Sayan Deb Sarkar</strong>, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni
                <!-- <br> -->
                <br>
                <a href="https://arxiv.org/pdf/2304.14880/pdf">arXiv</a> /        
                <a href="https://sayands.github.io/sgaligner/">Project Page</a> /
                <a href="https://youtu.be/nBTyZyY9X7I">Video</a> /
                <a href="https://github.com/sayands/sgaligner">Code</a>
                <br>
                <em> International Conference on Computer Vision (ICCV), 2023 </em>
                <br>
              
                <p></p>
                <p>
                  We focus on the fundamental problem of aligning pairs of 3D scene graphs whose overlap can range from
                  zero to partial and can contain arbitrary changes. We propose SGAligner, the first method for aligning 
                  pairs of 3D scene graphs that is robust to in-the-wild scenarios.
                  </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/handsformer_arvix.png' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation</papertitle>
                <br>
                Shreyas Hampali, <strong>Sayan Deb Sarkar</strong>, Mahdi Rad, Vincent Lepetit
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR), 2022 <strong>Oral</strong></em>
                <br>
                <a href="https://arxiv.org/pdf/2104.14639.pdf">arXiv</a> /        
                <a href="https://www.tugraz.at/index.php?id=57823">Project Page</a> /
                <a href="https://www.youtube.com/watch?v=D9YjoJnj_M4">Video</a> /
                <a href="https://github.com/shreyashampali/kypt_transformer">Code</a>
              
                <p></p>
                <p> We propose an efficient network architecture for estimating pose of two hands and object during complex interaction. We also release the challenging H2O-3D dataset, which contains two hands interacting with YCB objects. </p>
              </td>
            </tr> 

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/mcss_cvpr21.png' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Monte Carlo Scene Search for 3D Scene Understanding</papertitle>
                <br>
                Shreyas Hampali*, Sinisa Stekovic*, <strong>Sayan Deb Sarkar</strong>, Chetan Srinivasa Kumar, Friedrich Fraundorfer, Vincent Lepetit
                <br>
                <em>Computer Vision and Pattern Recognition (CVPR), 2021</em>
                <br>
                <a href="https://arxiv.org/pdf/2103.07969.pdf">arXiv </a> /
                <a href="https://www.tugraz.at/index.php?id=50484">Project Page</a> /
                <a href="https://www.youtube.com/watch?v=4kAfuymevUw&feature=youtu.be">Video</a> /
                <a href="https://github.com/montescene">Code</a>
                <p></p>
                <p> We propose a Monte-Carlo Tree Search (MCTS) based analysis-by-synthesis method to recover complete scene (3D layout+objects) from a RGB-D scan of the environment. <br> 
                 <em>*Equal contribution</em> </p>
              </td>
            </tr>

            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/roomlayout_eccv2020.png' width="160">
                </div>
                <script type="text/javascript">
                  function clipnerf_start() {
                    document.getElementById('clipnerf_image').style.opacity = "1";
                  }

                  function clipnerf_stop() {
                    document.getElementById('clipnerf_image').style.opacity = "0";
                  }
                  clipnerf_stop()
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>General 3D Room Layout from a Single View by Render-and-Compare</papertitle>
                <br>
                Sinisa Stekovic, Shreyas Hampali, Mahdi Rad, <strong>Sayan Deb Sarkar</strong>, Friedrich Fraundorfer, Vincent Lepetit
                <br>
                <em>European Conference on Computer Vision (ECCV), 2020</em>
                <br>
                <a href="https://arxiv.org/pdf/2001.02149.pdf">arXiv </a> /
                <a href="https://www.tugraz.at/institute/icg/research/team-lepetit/research-projects/general-3d-room-layout-from-a-single-view-by-render-and-compare/">Project Page</a> /
                <a href="https://youtu.be/ZLNnGNzzE7k">Video </a> /
                <a href="https://github.com/vevenom/RoomLayout3D_RandC">Code</a>
                <p></p>
                <p> We propose an analysis-by-synthesis method to estimate a 3D layout of the room - walls, floors, ceilings - from a single perspective view. The method recovers complex non-cubiod layouts by solving a constrained discrete optimization problem. </p>
              </td>
            </tr>

        </tbody> </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Course Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <a href="images/final_scene_cg_as22.png"><img src="images/final_scene_cg_as22.png" alt="prl" width="160"></a></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <p>
          <p>
            <a href="https://sayands.github.io/computer-graphics-project-report/">
            <papertitle>Ray Tracing</papertitle>
            </a>
            <br>
            <em> <a href="https://cgl.ethz.ch/teaching/cg22/home.php">Computer Graphics Rendering Competition</a>, Autumn Semester 2022 </em>
          </p>
          <p>
            Implemented a ray tracer with functionalities such as advanced camera models, participating media, photon mapping, Disney BRDF, etc on the 
            <strong>Nori</strong> framework. 
          </p>
          </p>
          </td>
        </tr>
        
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <p style="text-align:center;font-size:small;"> Design and code from <a href="https://jonbarron.info/"> Jon Barron's website</a></p>
              </td>
            </tr>
          </tbody>
        </table>
      </td>
    </tr>
  </table>


</body>

</html>

<script>
  function toggleBlock(blockId) {
    var block = document.getElementById(blockId);
    if (block.style.display === 'none') {
      block.style.display = 'block';
    } else {
      block.style.display = 'none';
    }
  }
</script>
